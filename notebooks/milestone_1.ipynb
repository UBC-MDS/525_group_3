{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 525 - Web and Cloud Computing\n",
    "\n",
    "## Milestone 1: Tackling big data on your laptop\n",
    "\n",
    "## Overall project goal and data \n",
    "\n",
    "During this course, you will be working on a team project involving big data. The purpose is to get exposure to working with much larger datasets than you have previously in MDS. You have been assigned to teams of three or four. (See group assignment in [Canvas](https://canvas.ubc.ca/courses/106517). Unlike previous project courses, in this course, all of you will be working on **the same problem**. In particular, you will be building and deploying ensemble machine learning models in the cloud to predict daily rainfall in Australia on a large dataset (~6 GB), where features are outputs of different climate models, and the target is the actual rainfall observation.  \n",
    "\n",
    "You will be using [this dataset on figshare](https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681). This folder has the output of different climate models as features, and our ultimate goal is to build an ensemble model on these outputs and compare the results with the actual rainfall. At the end of the project, you should have your ML model deployed in the cloud for others to use. \n",
    "\n",
    "During this course, you will work towards this goal step by step in four milestones.  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Milestone 1 checklist  \n",
    "\n",
    "\n",
    "\n",
    "### 1. Team-work contract\n",
    "rubric={correctness:10}\n",
    "\n",
    "Similar to what you did in DSCI 522 and DSCI 524, create a teamwork contract. The contract should outline how you are committed to working together so that you are accountable to one another. Again, you may start with your team contract document from previous project courses and adapt it to your new team. It is a fairly personal document, and please do not push it into your public repositories. Instead, save it somewhere your team can easily share it, and you can share a link to it or a copy with us in your submission to Canvas to prove you did this.\n",
    "\n",
    "### 2. Creating a repository and project structure \n",
    "rubric={mechanics:10}\n",
    "\n",
    "1. Similar to previous project courses, create a public repository under [UBC-MDS org](https://github.com/UBC-MDS) for your project. \n",
    "2. Write a brief introduction of the project in the `README`. \n",
    "3. Create a folder called `notebooks` in the repository and create a notebook for this milestone in that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "from pyarrow import csv\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Downloading the data \n",
    "rubric={correctness:10}\n",
    "\n",
    "1. Download the data from [figshare](https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681) to your local computer using the [figshare API](https://docs.figshare.com) (you need to make use of `requests` library).\n",
    "\n",
    "2. Extract the zip file, again programmatically, similar to how we did it in class. \n",
    "\n",
    ">  You can download the data and unzip it manually. But we learned about APIs, so we can do it in a reproducible way with the `requests` library, similar to how we [did it in class](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture1.html#using-rest-api-lab-lecture).\n",
    "\n",
    "> There are 5 files in the figshare repo. The one we want is: `data.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References code from Lecture notes: https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture1.html#using-rest-api-lab-lecture\n",
    "# article with daily rainfall data\n",
    "article_id = 14096681  \n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"figshare_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get request to get the files available\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  \n",
    "files = data[\"files\"]          \n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Download data.zip\n",
    "files_to_dl = [\"data.zip\"] \n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Extract Data.zip\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4. Combining data CSVs\n",
    "rubric={correctness:10,reasoning:10}\n",
    "\n",
    "1. Combine data CSVs into a single CSV using pandas.\n",
    "    \n",
    "2. When combining the CSV files, add an extra column called \"model\" that identifies the model.\n",
    "    Tip 1: you can get this column populated from the file name, eg: for file name \"SAM0-UNICON_daily_rainfall_NSW.csv\", the model name is SAM0-UNICON\n",
    "    Tip 2: Remember how we added \"year\" column when we combined airline CSVs. Here the regex will be to get word before an underscore ie, \"/([^_]*)\"\n",
    "\n",
    "> Note: There is a file called `observed_daily_rainfall_SYD.csv` in the data folder that you downloaded. Make sure you exclude this file (programmatically or just take out that file from the folder) before you combine CSVs. We will use this file in our next milestone.\n",
    "\n",
    "3. ***Compare*** run times on different machines within your team and summarize your observations. \n",
    "\n",
    "> Warning: Some of you might not be able to do it on your laptop. It's fine if you're unable to do it. Just make sure you discuss the reasons why you might not have been able to run this on your laptop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust based on your os\n",
    "%cd \"figshare_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Exclude the \"observed_daily_rainfall_SYD.csv\" file\n",
    "file_pattern = re.compile(r\"^(?!observed_daily_rainfall_SYD).*\\.csv$\")\n",
    "\n",
    "# Extract the model name\n",
    "def extract_model_name(file_name):\n",
    "    return re.match(r\"([^_]*)\", file_name).group(1)\n",
    "\n",
    "combined_data = []\n",
    "for file_name in os.listdir(\".\"):\n",
    "    if file_pattern.match(file_name):\n",
    "        model_name = extract_model_name(file_name)\n",
    "        print(model_name)\n",
    "        data = pd.read_csv(file_name)\n",
    "        data[\"model\"] = model_name\n",
    "        combined_data.append(data)\n",
    "\n",
    "# Combine all the dataframes\n",
    "combined_data = pd.concat(combined_data, ignore_index=True)\n",
    "combined_data.to_csv(\"combined_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "du -sh combined_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table for Q4\n",
    "data = {\n",
    "    \"Team Member\": [\"Andy Wang\", \"Samson Bakos\", \"Raul Aguilar\", \"Arjun Radhakrishnan\"],\n",
    "    \"Operating System\": [\"Windows 11\", \"MacOS Ventura 13.2\", \"MacOS Monterey 12.5.1\", \"Windows 11\"],\n",
    "    \"RAM\": [\"32GB\", \"16GB\", \"8GB\", \"16GB\"],\n",
    "    \"Processor\": [\"Intel(R) Core(TM) i7-10870H\", \"Apple M1\", \"Apple M2\", \"Intel(R) Core(TM) i7-12700H\"],\n",
    "    \"Is SSD\": [\"Y\", \"Y\", \"Y\", \"Y\"],\n",
    "    \"Time taken\": [\"6min 37s\", \"3min 55s\", \"3min 32s\", \"3 min 38s\"]\n",
    "}\n",
    "table = pd.DataFrame(data)\n",
    "\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Load the combined CSV to memory and perform a simple EDA\n",
    "rubric={correctness:10,reasoning:10}\n",
    "\n",
    "1. Investigate at least two of the following approaches to reduce memory usage while performing the EDA (e.g., value_counts). Refer to lecture notes [here](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture1.html#some-tactics-to-deal-with-memory-issue).\n",
    "    - Changing `dtype` of your data\n",
    "    - Load just columns that we want\n",
    "    - Loading in chunks\n",
    "    \n",
    "2. ***Compare*** run times on different machines within your team and summarize your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "columns = [\"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/day)\", \"model\"] \n",
    "# dropping Time because timestamp not useful for EDA\n",
    "# keeping the numeric columns and the model column for EDA\n",
    "\n",
    "df = pd.read_csv(\"combined_data.csv\", usecols = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Change float 64 to float 32\n",
    "float64_cols = list(df.select_dtypes(include='float64'))\n",
    "df[float64_cols] = df[float64_cols].astype('float32')\n",
    "\n",
    "# create column subsets to simplify computations\n",
    "numeric_cols= [\"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/day)\"]\n",
    "cat_cols = [\"model\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded size is {round(df.memory_usage().sum()*1e-9,2)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "df[cat_cols].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df[cat_cols].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df[numeric_cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table for q5\n",
    "data = {\n",
    "    \"Team Member\": [\"Samson Bakos\", \"Raul Aguilar\", \"Andy Wang\", \"Arjun Radhakrishnan\"],\n",
    "    \"Operating System\": [\"MacOS Ventura 13.2\", \"MacOS Monterey 12.5\", \"Windows 11\", \"Windows 11\"],\n",
    "    \"RAM\": [\"16GB\", \"8GB\", \"32GB\", \"16GB\"],\n",
    "    \"Processor\": [\"M1\", \"M2\", \"Intel(R) Core(TM) i7-10870H\", \"Intel(R) Core(TM) i7-12700H\"],\n",
    "    \"Is SSD\": [\"Y\", \"Y\", \"Y\", \"Y\"],\n",
    "    \"Time taken\": [\"51.9s\", \"49.1s\", \"66.95\", \"52.1s\"]\n",
    "}\n",
    "table = pd.DataFrame(data)\n",
    "\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### 6. Perform a simple EDA in R\n",
    "rubric={correctness:15,reasoning:10}\n",
    "\n",
    "1. Choose one of the methods listed below for transferring the dataframe (i.e., the entire dataset) from Python to R, and explain why you opted for this approach instead of the others.\n",
    "    - [Parquet file](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture2.html#converting-csv-parquet)\n",
    "    - [Pandas exchange](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture1.html#use-r-and-python-interchangeably)\n",
    "    - [Arrow exchange](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture2.html#use-r-and-python-interchangeably-with-arrow)\n",
    "2. Once you have the dataframe in R, perform a simple EDA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER__:\n",
    "\n",
    "One of the primary factors in our decision to choose `Arrow exchange` over `Parquet file` and `Pandas exchange` is its efficiency when it comes to serialization and de-serialization processes, allowing an optimal use of memory space. Additionally, since `Arrow exchange` uses an unified memory representation, it facilitates seamless interchangeability between multiple programming languages. Moreover, `Arrow R` has good integration with essential R libraries such as `Dplyr`, `Lubridate`, and `String R`. This integration empowers users to perform comprehensive data wrangling operations on large-scale data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your local path if necessary\n",
    "import os\n",
    "os.environ['R_HOME'] = '/Users/raulaguilar/opt/miniconda3/envs/525_dev/lib/R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython \n",
    "# Load R magic in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined data path\n",
    "combinedcsv = \"combined_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Build `pyarrow dataset`\n",
    "dataset = ds.dataset(combinedcsv, format=\"csv\")\n",
    "\n",
    "# Converting the `pyarrow dataset` to a `pyarrow table`\n",
    "table = dataset.to_table()\n",
    "\n",
    "# Converting a `pyarrow table` to a `rarrow table`\n",
    "r_table = pyra.converter.py2rpy(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R -i r_table\n",
    "\n",
    "# Check basic data structure\n",
    "suppressMessages({\n",
    "  library(dplyr)\n",
    "})\n",
    "\n",
    "glimpse <- r_table |>\n",
    "    glimpse() \n",
    "    \n",
    "print(glimpse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "# Records by model, top 10\n",
    "model_rows <- r_table |> \n",
    "    count(model) |> \n",
    "    arrange(desc(n)) |> \n",
    "    collect()\n",
    "    \n",
    "print(model_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Null count for target variable\n",
    "target_null <- r_table |>\n",
    "    filter(is.na(`rain (mm/day)`)) |> \n",
    "    count() |> \n",
    "    pull()\n",
    "    \n",
    "cat(\"There are\", target_null, \"null registers in 'rain' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "# summary statistics for numeric columns\n",
    "not_null <- r_table |>\n",
    "    select(lat_min, lat_max, lon_min, lon_max, `rain (mm/day)`) |> \n",
    "    filter(!is.na(`rain (mm/day)`)) |> \n",
    "    collect()\n",
    "        \n",
    "max_values <- sapply(not_null, max)\n",
    "min_values <- sapply(not_null, min)\n",
    "mean_values <- sapply(not_null, mean)\n",
    "sd_values <- sapply(not_null, sd)\n",
    "\n",
    "print(\"Mean values:\")\n",
    "print(mean_values)\n",
    "print(\"Min values:\")\n",
    "print(min_values)\n",
    "print(\"Max values:\")\n",
    "print(max_values)\n",
    "print(\"Sd values:\")\n",
    "print(sd_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific expectations for this milestone \n",
    "\n",
    "- In this milestone, we are looking for a well-documented and self-explanatory notebook that explores different options to tackle big data on your laptop.\n",
    "- Please discuss any challenges or difficulties you faced when dealing with this large amount of data on your laptop. You can stop combining the data if it takes more than 30 minutes. Briefly explain your approach to overcoming the challenges or reasons why you could not overcome them.\n",
    "- For questions 5 and 6, you are free to choose any exploratory data analysis (EDA) task you want. Visualization is not necessary; summarizing the data is enough. However, if you want to install additional packages for visualization that are not included in the .yml file, feel free to install them on top of your notebook. If you want to install packages in R, you can do so using `install.packages(\"dplyr\")` under `%%R` magic cell.\n",
    "- If someone in your team is facing issues with using R in a Python notebook, you can ignore it, as you will not need it for any other milestones. The main purpose of showing it in the lecture was to introduce and get a feel for the serialization and deserialization concept.\n",
    "- You only need to ***compare*** the time with other team members for questions 4 and 5. You do not need to do this for question 6. You can use the following table to record your results. Feel free to add any other relevant columns.\n",
    "\n",
    "\n",
    "| Team Member | Operating System | RAM | Processor | Is SSD | Time taken |\n",
    "|:-----------:|:----------------:|:---:|:---------:|:------:|:----------:|\n",
    "| Member 1    |                  |     |           |        |            |\n",
    "| Member 2    |                  |     |           |        |            |\n",
    "| Member 3    |                  |     |           |        |            |\n",
    "| Member 4    |                  |     |           |        |            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "rubric={mechanics:5}\n",
    "\n",
    "In the textbox provided on Canvas for the Milestone 1 assignment include:\n",
    "\n",
    "- The GitHub URL to your notebook.\n",
    "\n",
    "As comment include\n",
    "- Repo link\n",
    "- Teamwork contract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
